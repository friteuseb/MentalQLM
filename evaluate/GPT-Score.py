from openai import OpenAI
from functions import *
from datetime import datetime
import csv

# gpt-4o
# gpt-3.5-turbo
def openai_chat(system_prompt, user_prompt, model_name="gpt-4o"):
    client = OpenAI(
        base_url="gpt url",
        api_key="your api key"
    )
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
    )
    return response.choices[0].message.content


system_prompt = 'You are a mental health expert for checking the quality of the answer.'
user_prompt = ('[{}]\n[The Start of Assistant 1\'s Answer]\n{}\n'
               '[The End of Assistant 1\'s Answer]\n[The Start of Assistant 2\'s Answer]\n{}\n'
               '[The End of Assistant 2\'s Answer]\n')

requirement = ('We would like to request your feedback on the performance of two AI assistants in response to the user question' 
'displayed above.\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant' 
'receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\nPlease '
'first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The' 
' two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your' 
'evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not' 
'affect your judgment.')

# specify path of the rusult generated by different models 
qwen_path = 'generated_predictions_qwen.jsonl'
llama_path = 'generated_predictions_llama7.jsonl'

proportion = 200
dataset_name = 'SAD'
dataset_dict = {'DR': 50, 'dreaddit': 405, 'Irf': 819, 'MultiWD': 2932, 'SAD': 5373}
llama_list = read_json_dict(llama_path)[dataset_dict[dataset_name]:dataset_dict[dataset_name]+proportion]
qwen_list = read_json_dict(qwen_path)[dataset_dict[dataset_name]:dataset_dict[dataset_name]+proportion]
# specify path of the question list
question_list = read_json('.//mental_dataset//'
                          'mental_istct_test_'+dataset_name + '.json')[:proportion]
print("qwen_list_len:{}, llama_list_len:{}, truth_list_len:{}".format(len(qwen_list), len(llama_list), len(question_list)))

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
filename = f'output_{timestamp}.csv'

with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['qwen', 'llama', 'content'])

    for i in tqdm(range(0, len(llama_list)), desc="Scoring"):
        try:
            question = question_list[i]['instruct']
            llama_answer = llama_list[i]["predict"]
            qwen_answer = qwen_list[i]["predict"]

            new_user_prompt = user_prompt.format(question, qwen_answer, llama_answer)
            new_user_prompt += requirement

            answer = openai_chat(system_prompt, new_user_prompt)

            lines = answer.split('\n')
            qwen, llama = lines[0].split()

            content = '\n'.join(lines[1:])
            writer.writerow([qwen, llama, content])

        except Exception as e:
            print(f"Error processing question {i}: {e}")
            writer.writerow(['ERROR', 'ERROR', str(e)])


