{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction tuning dataset construction\n",
    "First, we need to construct the IMHI instruction tuning dataset tailored for the requirement of LLaMAFactory， consisting of the training set and validation set.\n",
    "Note, the constructed SAD dataset is used only for perplexity analysis.\n",
    "The SAD dataset used for instruction tuning is provided in the next block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T13:16:03.151784Z",
     "start_time": "2024-10-30T13:16:02.582418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to mental_istct_train_DR.json, length:1003\n",
      "Data has been saved to mental_istct_val_DR.json, length:430\n",
      "Data has been saved to mental_istct_train_dreaddit.json, length:2837\n",
      "Data has been saved to mental_istct_val_dreaddit.json, length:300\n",
      "Data has been saved to mental_istct_train_Irf.json, length:3943\n",
      "Data has been saved to mental_istct_val_Irf.json, length:985\n",
      "Data has been saved to mental_istct_train_MultiWD.json, length:15743\n",
      "Data has been saved to mental_istct_val_MultiWD.json, length:1500\n",
      "Data has been saved to mental_istct_train_SAD.json, length:5547\n",
      "Data has been saved to mental_istct_val_SAD.json, length:616\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dataset_list = ['DR', 'dreaddit', 'Irf', 'MultiWD', 'SAD']\n",
    "directory = \"mental_dataset//IMHI//train_data\"\n",
    "\n",
    "def write_train_dataset(dataset_name='DR'):\n",
    "    data_list_train = []\n",
    "    data_list_eval = []\n",
    "    for sub_directory in os.listdir(directory):\n",
    "        if sub_directory == dataset_name:\n",
    "            file_path = os.path.join(directory, sub_directory)\n",
    "            file_path_train = os.path.join(file_path, \"train.csv\")\n",
    "            file_path_val = os.path.join(file_path, \"val.csv\")\n",
    "            df_train = pd.read_csv(file_path_train)\n",
    "            df_val = pd.read_csv(file_path_val)\n",
    "\n",
    "            for index, row in df_train.iterrows():\n",
    "                data_dict = {\"instruct\": row[\"query\"], \"output\": row[\"gpt-3.5-turbo\"]}\n",
    "                data_list_train.append(data_dict)\n",
    "\n",
    "            for index, row in df_val.iterrows():\n",
    "                data_dict = {\"instruct\": row[\"query\"], \"output\": row[\"gpt-3.5-turbo\"]}\n",
    "                data_list_eval.append(data_dict)\n",
    "\n",
    "    json_data_train = json.dumps(data_list_train, indent=4)\n",
    "    json_data_eval = json.dumps(data_list_eval, indent=4)\n",
    "\n",
    "    with open(\".//mental_dataset//mental_istct_train_{}.json\".format(dataset_name), \"w\") as json_file:\n",
    "        json_file.write(json_data_train)\n",
    "    print(\"Data has been saved to mental_istct_train_{}.json, length:{}\".format(dataset_name, len(data_list_train)))\n",
    "    with open(\".//mental_dataset//mental_istct_val_{}.json\".format(dataset_name), \"w\") as json_file:\n",
    "        json_file.write(json_data_eval)\n",
    "    print(\"Data has been saved to mental_istct_val_{}.json, length:{}\".format(dataset_name, len(data_list_eval)))\n",
    "\n",
    "\n",
    "for i in dataset_list:\n",
    "    write_train_dataset(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the label in the output sentence for the SAD dataset and contruct the instruction tuning dataset for the SAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T06:15:54.552572Z",
     "start_time": "2024-11-05T06:15:54.373987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5547\n",
      "616\n",
      "Data has been saved to mental_istct_train_SAD_without_label.json\n",
      "Data has been saved to mental_istct_val_SAD_without_label.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "directory = \".//mental_dataset//IMHI//train_data\"\n",
    "\n",
    "\n",
    "def write_SAD_dataset(file_name=''):\n",
    "    data_list = []\n",
    "    for sub_directory in os.listdir(directory):\n",
    "        if sub_directory == 'SAD':\n",
    "            file_path = os.path.join(directory, sub_directory)        \n",
    "            file_path = os.path.join(file_path, file_name)  \n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            for index, row in df.iterrows():\n",
    "                instruct = row[\"query\"]\n",
    "                output = row[\"gpt-3.5-turbo\"]\n",
    "                output_an = output.split(\"Reasoning:\")[0]\n",
    "                reasoning = output.split(\"Reasoning:\")[1]\n",
    "\n",
    "                if 'school' in output_an.lower():\n",
    "                    label = 'school'\n",
    "                elif 'financial problem' in output_an.lower():\n",
    "                    label = 'financial problem'\n",
    "                elif 'family issues' in output_an.lower():\n",
    "                    label = 'family issues'\n",
    "                elif 'social relationships' in output_an.lower():\n",
    "                    label = 'social relationships'\n",
    "                elif 'work' in output_an.lower():\n",
    "                    label = 'work'\n",
    "                elif 'health issues' in output_an.lower():\n",
    "                    label = 'health issues'\n",
    "                elif 'emotion turmoil' in output_an.lower():\n",
    "                    label = 'emotion turmoil'\n",
    "                elif 'everyday decision making' in output_an.lower():\n",
    "                    label = 'everyday decision making'\n",
    "                elif 'other stress causes' in output_an.lower():\n",
    "                    label = 'other stress causes'\n",
    "\n",
    "                post = instruct.split(\"Question:\")[0]\n",
    "                question = \"Question: This post shows the stress cause related to {}, explain the reasoning of it step by step\".format(label)\n",
    "\n",
    "                data_dict = {\"instruct\": post+question, \"output\": reasoning}\n",
    "                data_list.append(data_dict)\n",
    "   \n",
    "    return data_list\n",
    "\n",
    "\n",
    "data_list_train = write_SAD_dataset('train.csv')\n",
    "print(len(data_list_train))\n",
    "data_list_eval = write_SAD_dataset('val.csv')\n",
    "print(len(data_list_eval))\n",
    "\n",
    "\n",
    "with open(\"./mental_dataset/mental_istct_train_SAD_without_label.json\", \"w\") as json_file:\n",
    "    json_data = json.dumps(data_list_train, indent=4)\n",
    "    json_file.write(json_data)\n",
    "    print('Data has been saved to mental_istct_train_SAD_without_label.json')\n",
    "\n",
    "with open(\"./mental_dataset/mental_istct_val_SAD_without_label.json\", \"w\") as json_file:\n",
    "    json_data = json.dumps(data_list_eval, indent=4)\n",
    "    json_file.write(json_data)\n",
    "    print('Data has been saved to mental_istct_val_SAD_without_label.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to construct the test set.\n",
    "First, we can visulize the input_length and output_length after vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the test set for the 'DR', 'dreaddit', 'Irf', and 'MultiWD'  subdataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T11:41:07.919149Z",
     "start_time": "2024-08-17T11:41:07.489311Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mental_istct_test_DR.json has been saved\n",
      "405\n",
      "mental_istct_test_dreaddit.json has been saved\n",
      "414\n",
      "mental_istct_test_Irf.json has been saved\n",
      "2113\n",
      "mental_istct_test_MultiWD.json has been saved\n",
      "2441\n",
      "mental_istct_test_SAD.json has been saved\n",
      "684\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "dataset_list = ['DR', 'dreaddit', 'Irf', 'MultiWD', 'SAD']\n",
    "directory = \"mental_dataset//IMHI//test_data\"\n",
    "\n",
    "def write_test_dataset(dataset_name):\n",
    "    data_list = []\n",
    "    file_name = dataset_name + '.csv'\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        data_dict = {\"instruct\": row[\"query\"], \"output\": row[\"gpt-3.5-turbo\"]}\n",
    "        data_list.append(data_dict)\n",
    "\n",
    "    json_data = json.dumps(data_list, indent=4)\n",
    "\n",
    "    with open(\".//mental_dataset//mental_istct_test_{}.json\".format(dataset_name), \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "    print(\"mental_istct_test_{}.json has been saved\".format(dataset_name))\n",
    "    print(len(data_list))\n",
    "\n",
    "\n",
    "for i in dataset_list:\n",
    "    write_test_dataset(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the test set for the 'SAD' subdataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T01:41:40.868444Z",
     "start_time": "2024-11-11T01:41:40.310482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mental_istct_test_SAD_without_label.json has been saved\n",
      "684\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "data_list = []\n",
    "test_dataset = 'SAD.csv'\n",
    "\n",
    "directory = \".//mental_dataset//IMHI//test_data\"\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name == test_dataset:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # 遍历DataFrame中的每一行，并将其转换为字典\n",
    "        for index, row in df.iterrows():\n",
    "            instruct = row[\"query\"]\n",
    "            output = row[\"gpt-3.5-turbo\"]\n",
    "            output_an = output.split(\"Reasoning:\")[0]\n",
    "            reasoning = output.split(\"Reasoning:\")[1]                \n",
    "            if 'school' in output_an.lower():\n",
    "                label = 'school'\n",
    "            elif 'financial problem' in output_an.lower():\n",
    "                label = 'financial problem'\n",
    "            elif 'family issues' in output_an.lower():\n",
    "                label = 'family issues'\n",
    "            elif 'social relationships' in output_an.lower():\n",
    "                label = 'social relationships'\n",
    "            elif 'work' in output_an.lower():\n",
    "                label = 'work'\n",
    "            elif 'health issues' in output_an.lower():\n",
    "                label = 'health issues'\n",
    "            elif 'emotion turmoil' in output_an.lower():\n",
    "                label = 'emotion turmoil'\n",
    "            elif 'everyday decision making' in output_an.lower():\n",
    "                label = 'everyday decision making'\n",
    "            elif 'other stress causes' in output_an.lower():\n",
    "                label = 'other stress causes'\n",
    "            \n",
    "            post = instruct.split(\"Question:\")[0]\n",
    "            question = \"Question: This post shows the stress cause related to {}, explain the reasoning of it step by step\".format(label)\n",
    "            data_dict = {\"instruct\": post+question, \"output\": reasoning}\n",
    "            data_list.append(data_dict)\n",
    "\n",
    "json_data = json.dumps(data_list, indent=4)\n",
    "\n",
    "with open(\".//mental_dataset//mental_istct_test_SAD_without_label.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "print(\"mental_istct_test_SAD_without_label.json has been saved\")\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to evaluate the results on a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T04:11:34.032324Z",
     "start_time": "2024-08-09T04:11:34.007370Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SAD': {'avg_accuracy': 63.89,\n",
       "  'weighted_f1': 61.41,\n",
       "  'micro_f1': 63.89,\n",
       "  'macro_f1': 59.39}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from functions import *\n",
    "\n",
    "dataset_list = ['DR', 'dreaddit', 'Irf', 'MultiWD', 'SAD']\n",
    "\n",
    "dataset_name = dataset_list[4]\n",
    "generated = {dataset_name: []}\n",
    "golden = {dataset_name: []}\n",
    "path = 'D://Mental-checkpoint//'+ str(dataset_name) + '.jsonl'\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip())\n",
    "        prompt = data['prompt']\n",
    "        label = data['label']\n",
    "        predict = data['predict']\n",
    "        \n",
    "        generated[dataset_name].append(predict)\n",
    "        \n",
    "        golden[dataset_name].append(label)\n",
    "\n",
    "calculate_f1(generated, golden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to evaluate the results on all the  sub-datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T05:57:53.233299Z",
     "start_time": "2024-07-31T05:57:53.082377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: DR, average acc:74.32, weighted F1 74.23, micro F1 74.32, macro F1 64.75, OOD count: 0\n",
      "\n",
      "Dataset: dreaddit, average acc:78.02, weighted F1 77.97, micro F1 78.02, macro F1 77.95, OOD count: 2\n",
      "\n",
      "Dataset: Irf, average acc:70.18, weighted F1 69.62, micro F1 70.18, macro F1 66.23, OOD count: 8\n",
      "\n",
      "Dataset: MultiWD, average acc:65.18, weighted F1 65.82, micro F1 65.18, macro F1 64.72, OOD count: 4\n",
      "\n",
      "Dataset: SAD, average acc:62.87, weighted F1 61.1, micro F1 62.87, macro F1 58.96, OOD count: 81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from functions import *\n",
    "\n",
    "# split the generated results based on different length of the datasets.\n",
    "dataset_list = ['DR', 'dreaddit', 'Irf', 'MultiWD', 'SAD']\n",
    "start_index = [0, 405, 819, 2932, 5373]\n",
    "end_index = [405, 819, 2932, 5373, 6057]\n",
    "generated = {}\n",
    "golden = {}\n",
    "dataset_name = \"generated_predictions\"\n",
    "\n",
    "path = 'E://python_prj_D//MentalLLaMA//output//bf16//'+ str(dataset_name) + '.jsonl'\n",
    "\n",
    "all_data = []\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip())\n",
    "        all_data.append(data)\n",
    "    \n",
    "for i, (start, end, name) in enumerate(zip(start_index, end_index, dataset_list)):\n",
    "    data = all_data[start:end]\n",
    "    generated[name] = []\n",
    "    golden[name] = []\n",
    "    for line in data:\n",
    "        prompt = line['prompt']\n",
    "        label = line['label']\n",
    "        predict = line['predict']\n",
    "        generated[name].append(predict)\n",
    "        golden[name].append(label)\n",
    "    \n",
    "calculate_f1(generated, golden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
